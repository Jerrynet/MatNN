function o = convolution(varargin)
%CONVOLUTION

o.name         = 'Convolution';
o.generateLoss = false;
o.setup        = @setup;
o.forward      = @forward;
o.backward     = @backward;


default_weight_param = {
            'name' {'', ''}         ... %empty names means use autogenerated name
    'enable_terms' [true, true]     ... 
       'generator' {@nn.generator.uniform, @nn.generator.constant} ...
       'generator_param' {[], []}   ... %default param
    'learningRate' single([1 1])    ...
     'weightDecay' single([1 1])
};
default_convolution_param = {
      'num_output' 1     ...
     'kernel_size' [3 3] ...
             'pad' [0 0] ...
          'stride' [1 1] ...
};

    function [resource, topSizes, param] = setup(l, bottomSizes)
        % resource only have .weight
        % if you have other outputs you want to save or share
        % you can set its learning rate to zero to prevent update


        if isfield(l, 'weight_param')
            wp1 = nn.utils.vararginHelper(default_weight_param, l.weight_param);
        else
            wp1 = nn.utils.vararginHelper(default_weight_param, default_weight_param);
        end
        if isfield(l, 'convolution_param')
            wp2 = nn.utils.vararginHelper(default_convolution_param, l.convolution_param);
        else
            wp2 = nn.utils.vararginHelper(default_convolution_param, default_convolution_param);
        end
        if ~any(wp1.enable_terms)
            error('At least enable one weight.');
        end

        assert(all(wp2.stride~=0));
        assert(numel(l.bottom)==1);
        assert(numel(l.top)==1);


        kernel_size = wp2.kernel_size;
        if numel(kernel_size) == 1
            kernel_size = [kernel_size, kernel_size];
        end
        stride_size = wp2.stride;
        if numel(stride_size) == 1
            stride_size = [stride_size, stride_size];
        end
        pad_size = wp2.pad;
        if numel(pad_size) == 1
            pad_size = [pad_size, pad_size, pad_size, pad_size];
        elseif numel(pad_size) == 2
            pad_size = [pad_size(1), pad_size(1), pad_size(2), pad_size(2)];
        end


        resource.weight = {[],[]};
        btmSize = bottomSizes{1};
        topSizes = {[floor([(btmSize(1)+pad_size(1)+pad_size(2)-kernel_size(1))/stride_size(1)+1, (btmSize(2)+pad_size(3)+pad_size(4)-kernel_size(2))/stride_size(2)+1]), wp2.num_output, btmSize(4)]};


        if wp1.enable_terms(1)
            resource.weight{1} = wp1.generator{1}([kernel_size(1), kernel_size(2), bottomSizes{1}(3), wp2.num_output], wp1.generator_param{1});
        end

        if wp1.enable_terms(2)
            resource.weight{2} = wp1.generator{2}([1, wp2.num_output], wp1.generator_param{2});
        end

        %return updated param
        param.weight_param = wp1;
        param.convolution_param = wp2;
    end


    function [top, weights, misc] = forward(opts, l, weights, misc, bottom, top)
        top{1} = vl_nnconv(bottom{1}, weights{1}, weights{2}, 'pad', l.convolution_param.pad, 'stride', l.convolution_param.stride);
    end


    function [bottom_diff, weights_diff, misc] = backward(opts, l, weights, misc, bottom, top, top_diff, weights_diff, weights_diff_isCumulate)
        %numel(bottom_diff) = numel(bottom), numel(weights_diff) = numel(weights)
        if weights_diff_isCumulate(1) && weights_diff_isCumulate(2)
            [ bottom_diff{1}, a, b ]= ...
                             vl_nnconv(bottom{1}, weights{1}, weights{2}, top_diff{1}, 'pad', l.convolution_param.pad, 'stride', l.convolution_param.stride);
            weights_diff{1} = weights_diff{1} + a;
            weights_diff{2} = weights_diff{2} + b;
        elseif weights_diff_isCumulate(1)
            [ bottom_diff{1}, outputdzdw, weights_diff{2} ]= ...
                             vl_nnconv(bottom{1}, weights{1}, weights{2}, top_diff{1}, 'pad', l.convolution_param.pad, 'stride', l.convolution_param.stride);
            weights_diff{1} = weights_diff{1} + outputdzdw;
        elseif weights_diff_isCumulate(2)
            [ bottom_diff{1}, weights_diff{1}, outputdzdw ]= ...
                             vl_nnconv(bottom{1}, weights{1}, weights{2}, top_diff{1}, 'pad', l.convolution_param.pad, 'stride', l.convolution_param.stride);
            weights_diff{2} = weights_diff{2} + outputdzdw;
        else
            [ bottom_diff{1}, weights_diff{1}, weights_diff{2} ]= ...
                             vl_nnconv(bottom{1}, weights{1}, weights{2}, top_diff{1}, 'pad', l.convolution_param.pad, 'stride', l.convolution_param.stride);
        end
    end

end
