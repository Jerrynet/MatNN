function [outBlob] = unsupervised_cnn(baseNet, blobNames)

assert(~isempty(baseNet));
assert(~isempty(blobNames));

no = nn.buildnet('unsupervised_cnn', baseNet);

batchSize = 128;
labelBlobName = 'inprod_origin';
dataBlobName  = 'data';
datapBlobName  = 'datap';

no.setDataBlobSize(dataBlobName , [1 784 1 batchSize]);
no.setDataBlobSize(datapBlobName , [1 784 1 batchSize]);
no.setDataBlobSize(labelBlobName, [1 1 1 batchSize]);


% ================================================ twins 1
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1'...
    'bottom' 'data' ...
    'top'    'conv1'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1'...
    'bottom' 'conv1'...
    'top'    'pool1'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2'...
    'bottom' 'pool1' ...
    'top'    'conv2'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2'...
    'bottom' 'conv2'...
    'top'    'pool2'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3'...
    'bottom' 'pool2' ...
    'top'    'conv3'...
    'convolution_param' {
        'num_output'  150 ...
        'kernel_size' [1 19]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1'...
    'bottom' 'conv3'...
    'top'    'relu1'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4'...
    'bottom' 'relu1' ...
    'top'    'fc4'...
    'convolution_param' {
        'num_output'  500 ...
        'kernel_size' [1 160]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'feat'...
    'bottom' 'fc4' ...
    'top'    'feat'...
    'convolution_param' {
        'num_output'  392 ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });

% ================================================ twins 2
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1p'...
    'bottom' 'datap' ...
    'top'    'conv1p'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1p'...
    'bottom' 'conv1p'...
    'top'    'pool1p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2p'...
    'bottom' 'pool1p' ...
    'top'    'conv2p'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2p'...
    'bottom' 'conv2p'...
    'top'    'pool2p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3p'...
    'bottom' 'pool2p' ...
    'top'    'conv3p'...
    'convolution_param' {
        'num_output'  150 ...
        'kernel_size' [1 19]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1p'...
    'bottom' 'conv3p'...
    'top'    'relu1p'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4p'...
    'bottom' 'relu1p' ...
    'top'    'fc4p'...
    'convolution_param' {
        'num_output'  500 ...
        'kernel_size' [1 160]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'featp'...
    'bottom' 'fc4p' ...
    'top'    'featp'...
    'convolution_param' {
        'num_output'  392  ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });

% ========================================inproduct of twins
no.newLayer({
    'type'   'layers.eltwise' ...
    'name'   'prod_twins' ...
    'bottom' {'feat', 'featp'} ...
    'top'    'prod_twins' ...
    'operation' 'prod'
    })

no.newLayer({
    'type'   'layers.convolution'...
    'name'   'inprod_twins'...
    'bottom' 'prod_twins' ...
    'top'    'inprod_twins'...
    'convolution_param' {
        'num_output'  1  ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'inprod_w', 'inprod_b'} ...
        'generator'    {@nn.generator.constant, @nn.generator.constant} ...
        'generator_param'    {{'value' 1}, {'value' 0}}...
        'learningRate' [0 0]
        }
    });

% =========================================loss
no.newLayer({
    'type'   'layers.loss.euclideanLoss'...
    'name'   'loss'...
    'bottom' {'inprod_twins', 'inprod_origin'}...
    'top'    'loss'
    });

[train4D, train4Dp, trainInProd] = readMNISTDataset('train-images-idx3-ubyte');

dataStruct  = nn.batch.generate(false, 'Name', dataBlobName,  'File', train4D, 'BatchSize', batchSize);
datapStruct = nn.batch.generate(false, 'Name', datapBlobName, 'File', train4Dp, 'BatchSize', batchSize);
labelStruct = nn.batch.generate(false, 'Name', labelBlobName, 'File', trainInProd, 'BatchSize', batchSize);
batchStruct = nn.batch.generate('Attach', dataStruct, datapStruct, labelStruct);

opts.numEpochs = [] ;
opts.numInterations = 1 ;
opts.numToTest = [];
opts.numToSave = []; %runs how many Epochs or iterations to save
opts.displayIter = [];
opts.batchSize = batchSize ;
opts.numSubBatches = 1 ;
opts.gpus = [1];
opts.computeMode = 'cuda kernel';

opts.continue = []; % if you specify the saving's iteration/epoch number, you can load it
opts.expDir = fullfile('data','exp') ;
opts.conserveMemory = false ;
opts.sync = false ;
opts.prefetch = false ;

[net_trained, outBlob] = nn.featextract(no, blobNames, batchStruct, opts);

end

function res = lrPolicy(currentBatchNumber, lr, gamma, power, steps)
     res = lr*((1+gamma*currentBatchNumber)^(-power));
end

function [train4D, train4Dp, trainInProd] = readMNISTDataset(trainImgFile)
    m = memmapfile(trainImgFile,'Offset', 16,'Format', {'uint8' [1 784] 'img'});
    imgData = m.Data;
    clearvars m;
    nTrain = numel(imgData);
    N = floor(nTrain/2);
    nTrain = 2*N;
    order = reshape(randperm(nTrain, nTrain), 2, N);
    train4D = zeros(784,N, 'single');
    train4Dp = zeros(784,N, 'single');
    trainInProd = zeros(1,N, 'single');
    for i=1:N
        train4D(:,i) = imgData(order(1,i)).img'/255;
        train4Dp(:,i) = imgData(order(2,i)).img'/255;
        trainInProd(:,i) = train4D(:,i)'*train4Dp(:,i);
    end

    train4D = reshape(train4D, [1 784 1 N]);
    train4Dp = reshape(train4Dp, [1 784 1 N]);
    trainInProd = reshape(trainInProd, [1 1 1 N]);

end