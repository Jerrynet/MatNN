function [net_trained] = mnist_siamese(baseNet)
if nargin == 1
    no = nn.buildnet('MNIST_siamese', baseNet);
else
    no = nn.buildnet('MNIST_siamese');
end

batchSize = 64;
labelBlobName = 'sim';
dataBlobName  = 'data';
datapBlobName  = 'datap';

no.setDataBlobSize(dataBlobName , [28 28 1 batchSize]);
no.setDataBlobSize(datapBlobName , [28 28 1 batchSize]);
<<<<<<< HEAD
no.setDataBlobSize(labelBlobName, [1  1  1 batchSize]);
=======
no.setDataBlobSize(labelBlobName, [1  1  1 batchSize]); 
>>>>>>> upstream/master


% ================================================ twins 1
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1'...
    'bottom' 'data' ...
    'top'    'conv1'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' 5  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1'...
    'bottom' 'conv1'...
    'top'    'pool1'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' 2  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2'...
    'bottom' 'pool1' ...
    'top'    'conv2'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' 5  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2'...
    'bottom' 'conv2'...
    'top'    'pool2'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' 2  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3'...
    'bottom' 'pool2' ...
    'top'    'conv3'...
    'convolution_param' {
        'num_output'  500 ...
        'kernel_size' 4  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1'...
    'bottom' 'conv3'...
    'top'    'relu1'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4'...
    'bottom' 'relu1' ...
    'top'    'fc4'...
    'convolution_param' {
        'num_output'  10 ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'feat'...
    'bottom' 'fc4' ...
    'top'    'feat'...
    'convolution_param' {
        'num_output'  2 ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });

% ================================================ twins 2
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1p'...
    'bottom' 'datap' ...
    'top'    'conv1p'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' 5  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1p'...
    'bottom' 'conv1p'...
    'top'    'pool1p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' 2  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2p'...
    'bottom' 'pool1p' ...
    'top'    'conv2p'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' 5  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2p'...
    'bottom' 'conv2p'...
    'top'    'pool2p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' 2  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3p'...
    'bottom' 'pool2p' ...
    'top'    'conv3p'...
    'convolution_param' {
        'num_output'  500 ...
        'kernel_size' 4  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1p'...
    'bottom' 'conv3p'...
    'top'    'relu1p'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4p'...
    'bottom' 'relu1p' ...
    'top'    'fc4p'...
    'convolution_param' {
        'num_output'  10 ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'featp'...
    'bottom' 'fc4p' ...
    'top'    'featp'...
    'convolution_param' {
        'num_output'  2  ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });


% =========================================loss
no.newLayer({
    'type'   'layers.loss.contrastiveLoss'...
    'name'   'loss'...
    'bottom' {'feat', 'featp', 'sim'}...
    'top'    'loss'
    });

[train4D, trainLabel, test4D, testLabel] = readMNISTDataset('train-images-idx3-ubyte', ...
                                                            'train-labels-idx1-ubyte', ...
                                                            't10k-images-idx3-ubyte', ...
                                                            't10k-labels-idx1-ubyte');

<<<<<<< HEAD
    function newInd = pairData(totalTimesOfDataSampled, lastErrorRateOfData, lastBatchIndices, N)
=======
    function newInd = pairData(totalTimesOfDataSampled, lastErrorRateOfData, lastBatchIndices, N) 
>>>>>>> upstream/master
        m = numel(totalTimesOfDataSampled);
        ro = randperm(m, 2*N);
        r = ro(1:N);
        rr = ro(N+1:end);
        labelInd = (trainLabel(r) == trainLabel(rr))+1;
        newInd = [r; rr; labelInd'];
    end

dataStruct  = nn.batch.generate(false, 'Name', dataBlobName,  'File', train4D, 'BatchSize', batchSize, 'Random', @pairData);
datapStruct = nn.batch.generate(false, 'Name', datapBlobName, 'File', train4D, 'BatchSize', batchSize, 'Random', @pairData);
labelStruct = nn.batch.generate(false, 'Name', labelBlobName, 'File', [0 1],   'BatchSize', batchSize, 'Random', @pairData, 'Using4D', false);
batchStruct = nn.batch.generate('Attach', dataStruct, datapStruct, labelStruct);

opts.numEpochs = [] ;
opts.numInterations = 50000 ;
opts.numToTest = 500;
opts.numToSave = 5000; %runs how many Epochs or iterations to save
opts.displayIter = 100;
opts.batchSize = batchSize ;
opts.numSubBatches = 1 ;
opts.gpus = [1];
opts.computeMode = 'cuda kernel';

opts.learningRate = 0.01 ;
opts.learningRatePolicy = @lrPolicy; %every iteration decays the lr
opts.learningRateGamma = 0.0001;
opts.learningRatePower = 0.75;
opts.weightDecay = 0.0000;

opts.continue = []; % if you specify the saving's iteration/epoch number, you can load it
opts.expDir = fullfile('data','exp') ;
opts.conserveMemory = false ;
opts.sync = false ;
opts.prefetch = false ;

[net_trained, batchStructTrained, ~] = nn.train(no, batchStruct, [], opts);


end

function res = lrPolicy(currentBatchNumber, lr, gamma, power, steps)
     res = lr*((1+gamma*currentBatchNumber)^(-power));
end

function [train4D, trainLabel, test4D, testLabel] = readMNISTDataset(trainImgFile, trainLabelFile, testImgFile, testLabelFile)
    m = memmapfile(trainImgFile,'Offset', 16,'Format', {'uint8' [28 28] 'img'});
    imgData = m.Data;
    clearvars m;
    train4D = zeros(28,28,1,numel(imgData), 'uint8');
    for i=1:numel(imgData)
        train4D(:,:,1,i) = imgData(i).img';
    end

    m = memmapfile(testImgFile,'Offset', 16,'Format', {'uint8' [28 28] 'img'});
    imgData = m.Data;
    clearvars m;
    test4D = zeros(28,28,1,numel(imgData), 'uint8');
    for i=1:numel(imgData)
        test4D(:,:,1,i) = imgData(i).img';
    end
<<<<<<< HEAD

    train4D = single(train4D)/255;
    test4D = single(test4D)/255;

=======
    
    train4D = single(train4D)/255;
    test4D = single(test4D)/255;
    
>>>>>>> upstream/master
    m = memmapfile(trainLabelFile,'Offset', 8,'Format', 'uint8');
    trainLabel = m.Data;
    m = memmapfile(testLabelFile, 'Offset', 8,'Format', 'uint8');
    testLabel = m.Data;
    clearvars m;

    % substract mean
    %trainMean = mean(train4D,4);
    %train4D = bsxfun(@minus, single(train4D), trainMean);
    %test4D = bsxfun(@minus, single(test4D), trainMean);

end