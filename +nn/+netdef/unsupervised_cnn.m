function [net_trained] = unsupervised_cnn(baseNet)
if nargin == 1
    no = nn.buildnet('unsupervised_cnn', baseNet);
else
    no = nn.buildnet('unsupervised_cnn');
end

batchSize = 50;
labelBlobName = 'inprod_origin';
dataBlobName  = 'data';
datapBlobName  = 'datap';

no.setDataBlobSize(dataBlobName , [1 784 1 batchSize]);
no.setDataBlobSize(datapBlobName , [1 784 1 batchSize]);
no.setDataBlobSize(labelBlobName, [1 1 1 batchSize]);


% ================================================ twins 1
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1'...
    'bottom' 'data' ...
    'top'    'conv1'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1'...
    'bottom' 'conv1'...
    'top'    'pool1'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2'...
    'bottom' 'pool1' ...
    'top'    'conv2'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2'...
    'bottom' 'conv2'...
    'top'    'pool2'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3'...
    'bottom' 'pool2' ...
    'top'    'conv3'...
    'convolution_param' {
        'num_output'  150 ...
        'kernel_size' [1 19]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1'...
    'bottom' 'conv3'...
    'top'    'relu1'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4'...
    'bottom' 'relu1' ...
    'top'    'fc4'...
    'convolution_param' {
        'num_output'  200 ...
        'kernel_size' [1 160]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'feat'...
    'bottom' 'fc4' ...
    'top'    'feat'...
    'convolution_param' {
        'num_output'  200 ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });

% ================================================ twins 2
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv1p'...
    'bottom' 'datap' ...
    'top'    'conv1p'...
    'convolution_param' {
        'num_output'  20 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv1_w1', 'conv1_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool1p'...
    'bottom' 'conv1p'...
    'top'    'pool1p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv2p'...
    'bottom' 'pool1p' ...
    'top'    'conv2p'...
    'convolution_param' {
        'num_output'  50 ...
        'kernel_size' [1 25]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv2_w1', 'conv2_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.pooling'...
    'name'   'pool2p'...
    'bottom' 'conv2p'...
    'top'    'pool2p'...
    'pooling_param' {
        'method'      'max' ...
        'kernel_size' [1 2]  ...
        'pad'         0  ...
        'stride'      2  ...
        }...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'conv3p'...
    'bottom' 'pool2p' ...
    'top'    'conv3p'...
    'convolution_param' {
        'num_output'  150 ...
        'kernel_size' [1 19]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'conv3_w1', 'conv3_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.relu'...
    'name'   'relu1p'...
    'bottom' 'conv3p'...
    'top'    'relu1p'...
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'fc4p'...
    'bottom' 'relu1p' ...
    'top'    'fc4p'...
    'convolution_param' {
        'num_output'  200 ...
        'kernel_size' [1 160]  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'fc4_w1', 'fc4_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });
no.newLayer({
    'type'   'layers.convolution'...
    'name'   'featp'...
    'bottom' 'fc4p' ...
    'top'    'featp'...
    'convolution_param' {
        'num_output'  200  ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'feat_w1', 'feat_b1'} ...
        'generator'    {@nn.generator.xavier, @nn.generator.constant} ...
        'learningRate' [1 2]
        }
    });

% ========================================inproduct of twins
no.newLayer({
    'type'   'layers.eltwise' ...
    'name'   'prod_twins' ...
    'bottom' {'feat', 'featp'} ...
    'top'    'prod_twins' ...
    'operation' 'prod'
    })

no.newLayer({
    'type'   'layers.convolution'...
    'name'   'inprod_twins'...
    'bottom' 'prod_twins' ...
    'top'    'inprod_twins'...
    'convolution_param' {
        'num_output'  1  ...
        'kernel_size' 1  ...
        'pad'         0  ...
        'stride'      1  ...
        }...
    'weight_param' {
        'name'         {'inprod_w', 'inprod_b'} ...
        'generator'    {@nn.generator.constant, @nn.generator.constant} ...
        'generator_param'    {{'value' 1}, {'value' 0}}...
        'learningRate' [0 0]
        }
    });

% =========================================loss
no.newLayer({
    'type'   'layers.loss.euclideanLoss'...
    'name'   'loss'...
    'bottom' {'inprod_twins', 'inprod_origin'}...
    'top'    'loss'
    });

[train4D, train4Dp, trainInProd, test4D, test4Dp, testInprod] = readMNISTDataset('train-images-idx3-ubyte', ...
                                                                                 't10k-images-idx3-ubyte');

dataStruct  = nn.batch.generate(false, 'Name', dataBlobName,  'File', train4D, 'BatchSize', batchSize);
datapStruct = nn.batch.generate(false, 'Name', datapBlobName, 'File', train4Dp, 'BatchSize', batchSize);
labelStruct = nn.batch.generate(false, 'Name', labelBlobName, 'File', trainInProd, 'BatchSize', batchSize);
batchStruct = nn.batch.generate('Attach', dataStruct, datapStruct, labelStruct);

opts.numEpochs = 100;
opts.numInterations = [];
opts.numToTest = [];
opts.numToSave = 5000; %runs how many Epochs or iterations to save
opts.displayIter = batchSize;
opts.batchSize = batchSize ;
opts.numSubBatches = 1 ;
opts.gpus = [1];
opts.computeMode = 'cuda kernel';

opts.learningRate = 0.000001;
opts.learningRatePolicy = @lrPolicy; %every iteration decays the lr
opts.learningRateGamma = 0.0001;
opts.learningRatePower = 0.75;
opts.weightDecay = 0.0002;

opts.continue = []; % if you specify the saving's iteration/epoch number, you can load it
opts.expDir = fullfile('data','exp') ;
opts.conserveMemory = false ;
opts.sync = false ;
opts.prefetch = false ;

[net_trained, batchStructTrained, ~] = nn.train(no, batchStruct, [], opts);

end

function res = lrPolicy(currentBatchNumber, lr, gamma, power, steps)
     res = lr*((1+gamma*currentBatchNumber)^(-power));
end

function [train4D, train4Dp, trainInProd, test4D, test4Dp, testInProd] = readMNISTDataset(trainImgFile, testImgFile)
    m = memmapfile(trainImgFile,'Offset', 16,'Format', {'uint8' [1 784] 'img'});
    imgData = m.Data;
    clearvars m;
    nTrain = numel(imgData);
    alldata = zeros(784, nTrain, 'single');
    for i=1:nTrain
        alldata(:,i) = imgData(i).img'/255;
    end
    alldata = bsxfun(@minus, alldata, mean(alldata,2));
    N = floor(nTrain/50);
    nTrain = 50*N;
    order = randperm(nTrain, N);
    train4D = alldata(:, 1:nTrain);
    train4Dp = zeros(784, nTrain, 'single');
    trainInProd = zeros(1,nTrain, 'single');
    for i=1:nTrain
        train4Dp(:,i) = alldata(:,order(ceil(i/50)));
        trainInProd(:,i) = train4D(:,i)'*train4Dp(:,i);
    end

    m = memmapfile(testImgFile,'Offset', 16,'Format', {'uint8' [1 784] 'img'});
    imgData = m.Data;
    clearvars m;
    nTest = numel(imgData);
    N = floor(nTest/50);
    nTest = 50*N;
    alldata = zeros(784, nTest, 'single');
    for i=1:nTest
        alldata(:,i) = imgData(i).img'/255;
    end;
    alldata = bsxfun(@minus, alldata, mean(alldata,2));
    N = floor(nTest/50);
    nTest = 50*N;
    order = randperm(nTest, N);
    test4D = alldata(:, 1:nTest);
    test4Dp = zeros(784, nTest, 'single');
    testInProd = zeros(1, nTest, 'single');
    for i=1:nTest
        test4Dp(:,i) = alldata(:,order(ceil(i/50)));
        testInProd(:,i) = test4D(:,i)'*test4Dp(:,i);
    end

    train4D = reshape(train4D, [1 784 1 nTrain]);
    train4Dp = reshape(train4Dp, [1 784 1 nTrain]);
    trainInProd = reshape(trainInProd, [1 1 1 nTrain]);

    test4D = reshape(test4D, [1 784 1 nTest]);
    test4Dp = reshape(test4Dp, [1 784 1 nTest]);
    testInProd = reshape(testInProd, [1 1 1 nTest]);

end